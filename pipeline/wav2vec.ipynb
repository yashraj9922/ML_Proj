{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae0f4f6b-11b6-48c5-b6a2-a0382c37929a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, RocCurveDisplay, f1_score, classification_report, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5de5105-6d38-40fc-bc07-13e332e538f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_audio_files_path = 'LA/LA/ASVspoof2019_LA_train/flac/'\n",
    "train_labels_path = 'LA/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.train.trn.txt.txt'\n",
    "len(os.listdir(train_audio_files_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a1667b0-e3f6-471f-a97b-ac0e0a73524b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readtxtfile(path):\n",
    "    with open(path, 'r') as file:\n",
    "        text = file.read().splitlines()\n",
    "        return text\n",
    "    \n",
    "def getlabels(path):\n",
    "    text = readtxtfile(path)\n",
    "    filename2label = {}\n",
    "    for item in tqdm(text):\n",
    "        key = item.split(' ')[1]\n",
    "        value = item.split(' ')[-1]\n",
    "        filename2label[key] = value\n",
    "        \n",
    "    return filename2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c094dc67-4b71-48e6-9102-9dd7f78bb50c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8634d4a420b4bd8bb56a17532c026e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filename2label = getlabels(train_labels_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e8e0ea3-cfed-4627-95e0-61868f10d5d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba37ae993da948ae97267009b183d133",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_audio_files_path = 'LA/LA/ASVspoof2019_LA_dev/flac/'\n",
    "val_labels_path = 'LA/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.dev.trl.txt.txt'\n",
    "val_filename2label = getlabels(val_labels_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed24ae6a-961d-4094-9c50-6cd86dd641f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([3., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       " array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "l = list(filename2label.values())\n",
    "print(l.count('bonafide'), l.count('spoof'))\n",
    "plt.hist(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d849c532-886c-4f90-a72a-dd695788245c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c65ace6a487479f9ae5d7d4a302a46a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_audio_files_path = 'LA/LA/ASVspoof2019_LA_eval/flac/'\n",
    "test_labels_path = 'LA/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.eval.trl.txt.txt'\n",
    "test_filename2label = getlabels(test_labels_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93510143-6549-4ccc-9ac2-0eaa9df48388",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASVSpoof(torch.utils.data.Dataset):\n",
    "    def __init__(self, audio_dir_path, num_samples, filename2label, transforms):\n",
    "        super().__init__()\n",
    "        self.audio_dir_path = audio_dir_path\n",
    "        self.num_samples = num_samples\n",
    "        self.audio_file_names = self.get_audio_file_names(filename2label)\n",
    "        self.labels, self.label2id, self.id2label = self.get_labels(filename2label)\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        signal, sr = torchaudio.load(os.path.join(self.audio_dir_path, self.audio_file_names[index]))\n",
    "#         print(signal.shape)\n",
    "        signal = self.mix_down_if_necessary(signal)\n",
    "        signal = self.cut_if_necessary(signal)\n",
    "#         print(signal.shape)\n",
    "        signal = self.right_pad_if_necessary(signal)\n",
    "#         print(signal.shape)\n",
    "#         signal = self.transforms(signal)\n",
    "#         print(signal.shape)\n",
    "        label = (self.labels[index])\n",
    "        return signal, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def get_audio_file_names(self, filename2label):\n",
    "        audio_file_names = list(filename2label.keys())\n",
    "        audio_file_names = [name + '.flac' for name in audio_file_names] # adding extension\n",
    "        return audio_file_names\n",
    "    \n",
    "    def get_labels(self, filename2label):\n",
    "        labels = list(filename2label.values())\n",
    "        id2label = {idx : label for idx, label in  enumerate(list(set(labels)))}\n",
    "        label2id = {label : idx for idx, label in  enumerate(list(set(labels)))}\n",
    "        labels = [label2id[label] for label in labels]\n",
    "        return labels, label2id, id2label\n",
    "    \n",
    "    def mix_down_if_necessary(self, signal): #converting from stereo to mono\n",
    "        if signal.shape[0] > 1: \n",
    "            signal = torch.mean(signal, dim = 0, keepdims = True)\n",
    "        return signal\n",
    "    \n",
    "    def cut_if_necessary(self, signal):\n",
    "        if signal.shape[1] > self.num_samples:\n",
    "            signal = signal[:, :num_samples]\n",
    "        return signal\n",
    "    \n",
    "    def right_pad_if_necessary(self, signal):\n",
    "        length = signal.shape[1]\n",
    "        if self.num_samples > length:\n",
    "            pad_last_dim = (0, num_samples - length)\n",
    "            signal = torch.nn.functional.pad(signal, pad_last_dim)\n",
    "        return signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b1e76a4-35ee-4294-a69d-e3b063677300",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 4 * 16000 # IMPORTANT!!\n",
    "train_dataset = ASVSpoof(train_audio_files_path, num_samples, filename2label, None)\n",
    "val_dataset = ASVSpoof(val_audio_files_path, num_samples, val_filename2label, None)\n",
    "test_dataset = ASVSpoof(test_audio_files_path, num_samples, test_filename2label, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "565056ba-db06-4a9a-b15e-70d7dce9d866",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "989ccdea-ffbd-46ac-8b48-3f5e5ef24839",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, shuffle = True, batch_size = 8)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, shuffle = True, batch_size = 8)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, shuffle = True, batch_size = 8)\n",
    "t_steps = len(train_loader)\n",
    "v_steps = len(val_loader)\n",
    "ts_steps = len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e294565-6688-44ea-aeff-4471c6afbd76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x25118175d60>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d61c177-34a7-4b2e-a069-f80033e5232e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Config, Wav2Vec2Processor, Wav2Vec2Model, Wav2Vec2FeatureExtractor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CustomWav2Vec2ForClassification(nn.Module):\n",
    "    def __init__(self, checkpoint):\n",
    "        super(CustomWav2Vec2ForClassification, self).__init__()\n",
    "        config = Wav2Vec2Config.from_pretrained(checkpoint)\n",
    "        self.feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(checkpoint)\n",
    "        print(self.feature_extractor)\n",
    "        self.wav2vec2 = Wav2Vec2Model.from_pretrained(checkpoint, config=config)\n",
    "        self.blstm = nn.LSTM(config.hidden_size, config.hidden_size // 2, bidirectional = True, num_layers = 2, batch_first = True)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.pool = nn.AdaptiveAvgPool1d(128)\n",
    "        self.linear = nn.Linear(199 * 128, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "\n",
    "        input_features = self.feature_extractor(input_ids, return_tensors=\"pt\", sampling_rate = 16000)\n",
    "        \n",
    "        ff = input_features.input_values\n",
    "        ff = ff.squeeze(0).to(device)\n",
    "\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        # Extract features from input audio\n",
    "        features = self.wav2vec2(ff, attention_mask=attention_mask, output_hidden_states = True).last_hidden_state\n",
    "        \n",
    "        # Pooling the last hidden states\n",
    "#         pooled_output = torch.mean(features, dim=1)  # Average pooling\n",
    "#         print(features.shape) # (batch_size, 199, 1024)\n",
    "        # Classification\n",
    "        res = features\n",
    "        x,(h, c) = self.blstm(features) # (batch_size, 199, 1024)\n",
    "#         print(x.shape)\n",
    "        x = x + res # (batch_size, 199, 1024)\n",
    "#         x = self.relu1(x)\n",
    "#         x = self.dropout1(x)\n",
    "        x = self.pool(x) #(batch_size, 199, 128)\n",
    "        x = x.view(x.shape[0], -1) #(batch_size, 199 * 128)\n",
    "#         print(x.shape)\n",
    "        x = self.linear(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bfc21f04-0dd2-42a0-9e46-6c59d4d1b357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wav2Vec2FeatureExtractor {\n",
      "  \"do_normalize\": true,\n",
      "  \"feature_extractor_type\": \"Wav2Vec2FeatureExtractor\",\n",
      "  \"feature_size\": 1,\n",
      "  \"padding_side\": \"right\",\n",
      "  \"padding_value\": 0.0,\n",
      "  \"return_attention_mask\": false,\n",
      "  \"sampling_rate\": 16000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'facebook/wav2vec2-base-960h'\n",
    "model = CustomWav2Vec2ForClassification(checkpoint)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-6)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6347704-1363-47e4-b532-1f246c261f58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "x = 100\n",
    "while(x != 0):\n",
    "    x = gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f88a0ea-71be-4c08-87b6-f2c89a415cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def EER(labels, outputs):\n",
    "    fpr, tpr, threshold = roc_curve(labels, outputs, pos_label=1)\n",
    "    fnr = 1 - tpr\n",
    "    eer_threshold = threshold[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "    eer_threshold\n",
    "    eer = fpr[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "    return eer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c5c02b2-e18a-45a5-b418-057ec59d591f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "996806680d0447ffb3a8a010cac1662f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad098d9d311c499a969af0cf3a1d71dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1 Training loss : 0.0 Train EER : 1.0 Validation loss : 0.0  Val EER : 1.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47a29b026939466bb66e9dd2d333da03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6779b3756f874d5697f426f728228740",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 2 Training loss : 0.0 Train EER : 1.0 Validation loss : 0.0  Val EER : 1.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f0931f1fa664c4aa177534ce89a96ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dae03833be1f4532b33a279108e39157",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 3 Training loss : 0.0 Train EER : 0.6666666666666666 Validation loss : 0.0  Val EER : 1.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4b171e77f454bb19ca5e45502f25fad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b234e17e8ade407bbbbe93515eb523cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 4 Training loss : 0.0 Train EER : 1.0 Validation loss : 0.0  Val EER : 1.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "731b361f5d4a40338c6f4f3e1463a33b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3edce749755e497e81eb484729eb0f1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 5 Training loss : 0.0 Train EER : 1.0 Validation loss : 0.0  Val EER : 1.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "473ecb35f0dd4985859a60fe59642b6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca5fc7777f6243f5a06f51a6b1e0e533",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 6 Training loss : 0.0 Train EER : 0.3333333333333333 Validation loss : 0.0  Val EER : 1.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ced69ae089d14e898e7f3a4de07cac86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ea8632a837b4d17a18c22330b89fa7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 7 Training loss : 0.0 Train EER : 1.0 Validation loss : 0.0  Val EER : 1.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8369e8598fa64c3d9b6fcfbc1dcfa03b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bee10e8529344cba10eea7702d9af53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 8 Training loss : 0.0 Train EER : 1.0 Validation loss : 0.0  Val EER : 1.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "783825ab7bdd4ab8af0d38b9c5643636",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4584511b40b54bf196f7906eb326031d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 9 Training loss : 0.0 Train EER : 1.0 Validation loss : 0.0  Val EER : 1.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bb909d428704097bfff437e39982e49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b091aa0e6b6d4337a786bd805784a973",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 10 Training loss : 0.0 Train EER : 1.0 Validation loss : 0.0  Val EER : 1.0\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 10\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "torch.cuda.empty_cache()\n",
    "for epoch in range(num_epochs):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    train_loss = 0.0\n",
    "    loop = tqdm(enumerate(train_loader), total = len(train_loader))\n",
    "    for batch_idx, (input_ids, labels) in loop:\n",
    "        loop.set_description(f'Epoch {epoch + 1} / {num_epochs} ')\n",
    "#         forward pass\n",
    "        model.train()\n",
    "        torch.cuda.empty_cache()\n",
    "        input_ids = input_ids.to(device)\n",
    "        input_ids = input_ids.squeeze(1)\n",
    "        labels = labels.to(device)\n",
    "        labels = labels.to(device).reshape(-1, 1)\n",
    "        labels = labels.type(torch.FloatTensor)\n",
    "        attention_mask = torch.ones(input_ids.shape, dtype=torch.long).to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        y_true.append(labels.detach().cpu().numpy())\n",
    "        y_pred.append(outputs.detach().cpu().numpy())\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        train_loss += loss.item()\n",
    "#         backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loop.set_postfix(Training_loss = loss.item())\n",
    "    \n",
    "    y_true = np.concatenate(y_true)\n",
    "    y_pred = np.concatenate(y_pred)\n",
    "    train_eer = EER(y_true, y_pred)\n",
    "        \n",
    "#   validation every epoch\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        val_loop = tqdm(enumerate(val_loader), total = len(val_loader))\n",
    "        for val_batch_idx, (val_input_ids, val_labels) in val_loop:\n",
    "            torch.cuda.empty_cache()\n",
    "            val_input_ids = val_input_ids.to(device)\n",
    "            val_input_ids = val_input_ids.squeeze(1)\n",
    "            val_labels = val_labels.to(device)\n",
    "            val_labels = val_labels.to(device).reshape(-1, 1)\n",
    "            val_labels = val_labels.type(torch.FloatTensor) #use torch.FloatTensor if on cpu\n",
    "            attention_mask = torch.ones(val_input_ids.shape, dtype=torch.long).to(device)\n",
    "        \n",
    "        \n",
    "            val_outputs = model(val_input_ids, attention_mask)\n",
    "            y_true.append(val_labels.detach().cpu().numpy())\n",
    "            y_pred.append(val_outputs.detach().cpu().numpy())\n",
    "            curr_val_loss = criterion(val_outputs, val_labels)\n",
    "            val_loss += curr_val_loss.item()\n",
    "            val_loop.set_postfix(validation_loss = curr_val_loss.item())\n",
    "            \n",
    "    train_loss_after_epoch = train_loss / t_steps\n",
    "    val_loss_after_epoch = val_loss / v_steps\n",
    "    train_losses.append(train_loss_after_epoch)\n",
    "    val_losses.append(val_loss_after_epoch)\n",
    "    y_true = np.concatenate(y_true)\n",
    "    y_pred = np.concatenate(y_pred)\n",
    "    val_eer = EER(y_true, y_pred)\n",
    "    print(f'Epoch : {epoch + 1} Training loss : {train_loss_after_epoch} Train EER : {train_eer} Validation loss : {val_loss_after_epoch}  Val EER : {val_eer}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c36bca3-7516-4670-be51-7f9c5329af18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fba6a257b18b46bc8abe8a81d93e27df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_outputs = []\n",
    "new_labels = []\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    test_loop = tqdm(enumerate(test_loader), total = len(test_loader))\n",
    "    for test_batch_idx, (test_input_ids, test_labels) in test_loop:\n",
    "        torch.cuda.empty_cache()\n",
    "        test_input_ids = test_input_ids.to(device)\n",
    "        test_input_ids = test_input_ids.squeeze(1)\n",
    "        test_labels = test_labels.to(device)\n",
    "        test_labels = test_labels.to(device).reshape(-1, 1)\n",
    "        test_labels = test_labels.type(torch.FloatTensor) #use torch.FloatTensor if on cpu\n",
    "        attention_mask = torch.ones(test_input_ids.shape, dtype=torch.long).to(device)\n",
    "\n",
    "\n",
    "        test_outputs = model(test_input_ids, attention_mask)\n",
    "        new_outputs.append(test_outputs.cpu().numpy())\n",
    "        new_labels.append(test_labels.cpu().numpy())\n",
    "        curr_test_loss = criterion(test_outputs, test_labels)\n",
    "        test_loss += curr_test_loss.item()\n",
    "        test_loop.set_postfix(test_loss = curr_test_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f24b206-0d9b-4ae3-a21a-81b5664c1753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 1) (6, 1)\n"
     ]
    }
   ],
   "source": [
    "new_labels = np.concatenate(new_labels)\n",
    "new_outputs = np.concatenate(new_outputs)\n",
    "print(new_labels.shape, new_outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "55bb9c0d-8f9c-4bb6-bb66-f34ac6f4e328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.50561637"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpr, tpr, threshold = roc_curve(new_labels, new_outputs, pos_label=1)\n",
    "fnr = 1 - tpr\n",
    "eer_threshold = threshold[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "eer_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f2d129c-3509-4f24-86a1-d7a71b03f391",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EER = fpr[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "EER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6721133f-6f38-402b-bbed-6ed13db51f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'wav2vec2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69dc5ec-f761-42c7-b8db-0561145f1dc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
